<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="preload" href="/_next/static/media/bb3ef058b751a6ad-s.p.woff2" as="font" crossorigin="" type="font/woff2"/><link rel="preload" href="/_next/static/media/e4af272ccee01ff0-s.p.woff2" as="font" crossorigin="" type="font/woff2"/><link rel="stylesheet" href="/_next/static/css/8aed5db9fe48a3cd.css" data-precedence="next"/><link rel="stylesheet" href="/_next/static/css/60da0364ed65e94f.css" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/_next/static/chunks/webpack-d0ceb3009605d4ea.js"/><script src="/_next/static/chunks/fd9d1056-e8d620389350fe1d.js" async=""></script><script src="/_next/static/chunks/23-7f7cc880f24b0617.js" async=""></script><script src="/_next/static/chunks/main-app-ed5c98195306a9c3.js" async=""></script><script src="/_next/static/chunks/231-9f01fd254defc16e.js" async=""></script><script src="/_next/static/chunks/app/blog/%5Bslug%5D/page-5eca7ec39609f401.js" async=""></script><title>Curie Research Platform</title><link rel="icon" href="/favicon.ico" type="image/x-icon" sizes="200x200"/><meta name="next-size-adjust"/><script src="/_next/static/chunks/polyfills-78c92fac7aa8fdd8.js" noModule=""></script></head><body class="__variable_e8ce0c __variable_3c557b antialiased"><article class="min-h-screen bg-stone-600"><header class="bg-stone-600"><div class="max-w-4xl mx-auto px-4 sm:px-6 lg:px-8 pt-16"><div class="mb-4"><a class="text-orange-400 font-medium" href="/">Home</a></div><h1 class="text-4xl sm:text-5xl font-bold text-white mb-6">Can AI Conduct AI Research Experiments?</h1><div class="flex items-center gap-4 text-white mb-6"><time dateTime="Tue Jun 10 2025 17:00:00 GMT-0700 (Pacific Daylight Time)">June 10, 2025</time></div><div class="flex gap-2 flex-wrap mb-6"><span class="px-3 py-1 bg-gray-200 text-gray-700 hover:bg-gray-300 text-sm rounded-full">machine-learning</span><span class="px-3 py-1 bg-gray-200 text-gray-700 hover:bg-gray-300 text-sm rounded-full">research</span></div></div></header><div class="prose prose-lg max-w-4xl mx-auto markdown-content"><div><div style="text-align: center">


<p style="font-size: 0.8em; color: white; margin-bottom: 2em;">
  <a href="https://www.cs-pk.com/">Patrick Tser Jern Kon</a><sup>*</sup>, 
  <a href="https://websites.umich.edu/~amberljc/">Jiachen Liu</a><sup>*</sup> (Equal Contribution<sup>*</sup>)<br>
  Xinyi Zhu,
  Qiuyi Ding, 
  <a href="https://www.linkedin.com/in/jingjia-peng/">Jingjia Peng</a> (University of Michigan),  
  <a href="https://jxing.me/">Jiarong Xing</a> (UC Berkeley &amp; Rice University), 
  <a href="https://huangyibo.github.io/">Yibo Huang</a> (University of Michigan), 
  <a href="https://yimingqiu.me/">Yiming Qiu</a> (UC Berkeley &amp; University of Michigan), 
  <a href="https://scholar.google.com/citations?user=HtNfeKYAAAAJ&amp;hl=en">Jayanth Srinivasa</a>,
  <a href="https://www.linkedin.com/in/myungjin-lee-5308136/">Myungjin Lee</a> (Cisco Research), <br>
  <a href="https://www.mosharaf.com/">Mosharaf Chowdhury</a> (University of Michigan), 
  <a href="https://stanford.edu/~matei/">Matei Zaharia</a> (UC Berkeley), 
  <a href="https://web.eecs.umich.edu/~chenang/">Ang Chen</a> (University of Michigan)
</p>
</div>

<div style="text-align: center; margin-bottom: 2em;">
  <div style="margin-bottom: 2em;">
    <a href="https://github.com/Just-Curieous/Curie/tree/main/benchmark/exp_bench" style="margin: 0 10px; padding: 8px 15px; background-color: #f8f9fa; border-radius: 5px; text-decoration: none; color: #2c3e50;">📊 Dataset</a>
    <a href="https://arxiv.org/abs/2505.24785" style="margin: 0 10px; padding: 8px 15px; background-color: #f8f9fa; border-radius: 5px; text-decoration: none; color: #2c3e50;">📄 Paper</a>
    <a href="https://github.com/Just-Curieous/Curie" style="margin: 0 10px; padding: 8px 15px; background-color: #f8f9fa; border-radius: 5px; text-decoration: none; color: #2c3e50;">💻 Github</a>
  </div>
</div>


<p>AI for Science is rapidly advancing, with promising early work on scientific automation—such as DeepMind's <a href="https://deepmind.google/discover/blog/alphaevolve-a-gemini-powered-coding-agent-for-designing-advanced-algorithms/">AlphaEvolve</a> and others highlighted in this Nature <a href="https://www.nature.com/articles/s41586-023-06221-2">paper</a>.
A particularly exciting frontier is <strong>the automation of AI research experimentation</strong>—the process of designing, executing, and analyzing experiments to advance AI itself. Unlike fields requiring physical experimentation, AI research is largely digital—well-suited for LLM-based automation. </p>
  <div style="padding: 20px; border-radius: 8px; margin: 2em 0;">
    <p style="margin-bottom: 1em;">Ideally, we want to provide an AI agent with a research goal—such as reproducing a result, validating a new hypothesis, or testing an ablation— along with the specific context and have the agent:</p>
    <ul style="list-style-type: none; padding-left: 0;">
      <li style="margin-bottom: 0.5em;">• Formulate hypotheses, design experiments,</li>
      <li style="margin-bottom: 0.5em;">• Interpret the associated codebase and identify how to modify it,</li>
      <li style="margin-bottom: 0.5em;">• Configure and execute experiments under the right conditions,</li>
      <li style="margin-bottom: 0.5em;">• Analyze results and iteratively refine its approach based on findings</li>
    </ul>
  </div>

<div style="text-align: center; max-width: 800px; margin: 0 auto; padding: 15px;">
    <img src="../blog/exp-bench-overview.png" alt="EXP-Bench Overview" style="width: 100%; height: auto; display: block; margin: 0 auto;">
    <p style="font-size: 0.8em; margin-top: 10px; color: white;">
        Figure 1. EXP-Bench evaluates AI agents on research experiment tasks extracted semi-autonomously from peer-reviewed AI papers.
    </p>
</div>
Achieving this vision requires benchmarks that evaluate agents in real-world research scenarios. But how do we define those scenarios in a way that’s representative, reproducible, and gradable?



<p>Intuitively, peer-reviewed AI papers (e.g., in NeurIPS) along with their open-source codebases, offer a rich source of completed experiments that could, in theory, be repurposed to evaluate AI capabilities in research automation. In practice, however, extracting these tasks is difficult. Papers often present a polished narrative that omits intermediate steps, while critical details—such as the precise conditions under which results hold—are scattered across dense text, supplementary materials, and sprawling repositories.</p>
<h2>Our Contribution: EXP-Bench</h2>
<p>To address this challenge, we introduce <strong>EXP-Bench</strong>, a new benchmark designed to make the ever-expanding landscape of published research more accessible for evaluating AI agents in <strong>conducting end-to-end AI research experiments</strong>—from hypothesis to experimental setup to conclusion, as shown in Figure 1. We develop a semi-automated pipeline (Figure 2) that uses multimodal and agentic approaches to reconstruct experiments from fragmented and dense sources (e.g., coding agents identify setups by conditioning on ground-truth outcomes and leveraging the full codebase—reducing the task to a constrained search), while interleaving these steps with lightweight human validation to ensure correctness.</p>
<p>Using this approach, we distilled <em>461 experiments from NeurIPS and ICLR papers</em>—spanning domains such as vision, RL, and computational biology—resulting in over 12,000 gradable subtasks.</p>
<div style="text-align: center; max-width: 800px; margin: 0 auto; padding: 15px;">
    <img src="../blog/exp-construction.png" alt="EXP-Bench Construction" style="width: 100%; height: auto; display: block; margin: 0 auto;">
    <p style="font-size: 0.8em; margin-top: 10px; color: white;">
        Figure 2. Our semi-automated pipeline for constructing EXP-Bench from published papers.
    </p>
    <p style="font-size: 0.8em; margin-top: 15px; color: white;"> 
            See Figure 3 for an example AI research task extracted through this pipeline.
    </p>

</div>
<div style="text-align: center; max-width: 800px; margin: 0 auto; padding: 0 15px;"> 
    <img src="../blog/exp-bench-example.png" alt="EXP-Bench Example" style="width: 100%; height: auto; display: block; margin: 0 auto;">
    <p style="font-size: 0.8em; margin-top: 10px; color: white;">
        Figure 3. One AI research task example from ICLR 2024 MogaNet [1].
    </p>
</div>

<h2>What EXP-Bench Reveals About Today's AI Agents</h2>
<p>We tested leading agents, including <em>OpenHands w/ Claude Sonnet 3.7</em>, and found that while they can earn partial credit for individual steps like experiment design or coding (~20-35% success), their ability to <em>complete a full, executable experiment</em> is nearly non-existent—a mere 0.5% success rate. 
  </p><div style="padding: 20px; border-radius: 8px; margin: 2em 0;">
    <p style="margin-bottom: 1em;">Our analysis pinpointed several critical weaknesses:</p>
    <ol style="padding-left: 20px;">
      <li style="margin-bottom: 0.5em;"><em>Limited Long-Horizon Planning and Reasoning</em></li>
      <li style="margin-bottom: 0.5em;"><em>Inability to Handle Open-Ended and Ambiguous Tasks (35.9%)</em></li>
      <li style="margin-bottom: 0.5em;"><em>Difficulty with Environment Setup (41.3%) and Code Debugging (29.8%)</em></li>
    </ol>
  </div><p></p>
<p>These results highlight just how far we still are from our goal of automation of research experimentation. By identifying these bottlenecks and providing realistic step-by-step experiment procedures, EXP-Bench serves as a vital tool for future AI agents to improve their ability to conduct AI research experiments.</p>
<h2>Looking Forward</h2>
<p>This work is, we hope, a small step toward our broader goal of designing agents capable of automating scientific research. We see EXP-Bench as a launchpad for the next wave of AI research copilots.</p>
<p>That said, much work remains. While EXP-Bench currently focuses on machine learning papers, it does not yet address domains that require interaction with the physical world or support tasks involving true scientific invention. Expanding to those areas—and capturing the creativity, uncertainty, and iteration of real-world discovery—remains an open and exciting challenge.</p>
<h2>Explore Our Work</h2>
  <div style="display: flex; justify-content: center; gap: 20px; margin: 2em 0;">
    <a href="https://arxiv.org/abs/2505.24785" style="padding: 10px 20px; background-color: #f8f9fa; color: #2c3e50; text-decoration: none; border-radius: 5px;">📑 Full paper</a>
    <a href="https://github.com/Just-Curieous/Curie/tree/main/benchmark/exp_bench" style="padding: 10px 20px; background-color: #f8f9fa; color: #2c3e50; text-decoration: none; border-radius: 5px;">🗃️ Open-sourced dataset</a>
  </div>

<pre><code class="language-latex hljs" data-highlighted="yes">@article{kon2025expbenchaiconductai,
      title={EXP-Bench: Can AI Conduct AI Research Experiments?}, 
      author={Patrick Tser Jern Kon and Jiachen Liu and Xinyi Zhu and Qiuyi Ding and Jingjia Peng and Jiarong Xing and Yibo Huang and Yiming Qiu and Jayanth Srinivasa and Myungjin Lee and Mosharaf Chowdhury and Matei Zaharia and Ang Chen},
      journal={arXiv preprint 2505.24785}
      year={2025},
}
</code></pre>
<p><em>[1] MogaNet: Multi-order Gated Aggregation Network. ICLR 2024</em></p>
</div></div></article><script src="/_next/static/chunks/webpack-d0ceb3009605d4ea.js" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0]);self.__next_f.push([2,null])</script><script>self.__next_f.push([1,"1:HL[\"/_next/static/media/bb3ef058b751a6ad-s.p.woff2\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff2\"}]\n2:HL[\"/_next/static/media/e4af272ccee01ff0-s.p.woff2\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff2\"}]\n3:HL[\"/_next/static/css/8aed5db9fe48a3cd.css\",\"style\"]\n4:HL[\"/_next/static/css/60da0364ed65e94f.css\",\"style\"]\n"])</script><script>self.__next_f.push([1,"5:I[5751,[],\"\"]\n8:I[9275,[],\"\"]\na:I[1343,[],\"\"]\nc:I[6130,[],\"\"]\n9:[\"slug\",\"2025-06-11-exp-bench-can-ai-conduct-ai-research-experiments\",\"d\"]\nd:[]\n"])</script><script>self.__next_f.push([1,"0:[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/8aed5db9fe48a3cd.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\"}]],[\"$\",\"$L5\",null,{\"buildId\":\"NwW2mHSQYi0zOhpz4EJUD\",\"assetPrefix\":\"\",\"initialCanonicalUrl\":\"/blog/2025-06-11-exp-bench-can-ai-conduct-ai-research-experiments\",\"initialTree\":[\"\",{\"children\":[\"blog\",{\"children\":[[\"slug\",\"2025-06-11-exp-bench-can-ai-conduct-ai-research-experiments\",\"d\"],{\"children\":[\"__PAGE__?{\\\"slug\\\":\\\"2025-06-11-exp-bench-can-ai-conduct-ai-research-experiments\\\"}\",{}]}]}]},\"$undefined\",\"$undefined\",true],\"initialSeedData\":[\"\",{\"children\":[\"blog\",{\"children\":[[\"slug\",\"2025-06-11-exp-bench-can-ai-conduct-ai-research-experiments\",\"d\"],{\"children\":[\"__PAGE__\",{},[[\"$L6\",\"$L7\"],null],null]},[\"$\",\"$L8\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\",\"blog\",\"children\",\"$9\",\"children\"],\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$La\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"notFoundStyles\":\"$undefined\",\"styles\":[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/60da0364ed65e94f.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\"}]]}],null]},[\"$\",\"$L8\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\",\"blog\",\"children\"],\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$La\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"notFoundStyles\":\"$undefined\",\"styles\":null}],null]},[[\"$\",\"html\",null,{\"lang\":\"en\",\"children\":[\"$\",\"body\",null,{\"className\":\"__variable_e8ce0c __variable_3c557b antialiased\",\"children\":[\"$\",\"$L8\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\"],\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$La\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[[\"$\",\"title\",null,{\"children\":\"404: This page could not be found.\"}],[\"$\",\"div\",null,{\"style\":{\"fontFamily\":\"system-ui,\\\"Segoe UI\\\",Roboto,Helvetica,Arial,sans-serif,\\\"Apple Color Emoji\\\",\\\"Segoe UI Emoji\\\"\",\"height\":\"100vh\",\"textAlign\":\"center\",\"display\":\"flex\",\"flexDirection\":\"column\",\"alignItems\":\"center\",\"justifyContent\":\"center\"},\"children\":[\"$\",\"div\",null,{\"children\":[[\"$\",\"style\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}\"}}],[\"$\",\"h1\",null,{\"className\":\"next-error-h1\",\"style\":{\"display\":\"inline-block\",\"margin\":\"0 20px 0 0\",\"padding\":\"0 23px 0 0\",\"fontSize\":24,\"fontWeight\":500,\"verticalAlign\":\"top\",\"lineHeight\":\"49px\"},\"children\":\"404\"}],[\"$\",\"div\",null,{\"style\":{\"display\":\"inline-block\"},\"children\":[\"$\",\"h2\",null,{\"style\":{\"fontSize\":14,\"fontWeight\":400,\"lineHeight\":\"49px\",\"margin\":0},\"children\":\"This page could not be found.\"}]}]]}]}]],\"notFoundStyles\":[],\"styles\":null}]}]}],null],null],\"couldBeIntercepted\":false,\"initialHead\":[null,\"$Lb\"],\"globalErrorComponent\":\"$c\",\"missingSlots\":\"$Wd\"}]]\n"])</script><script>self.__next_f.push([1,"e:I[231,[\"231\",\"static/chunks/231-9f01fd254defc16e.js\",\"308\",\"static/chunks/app/blog/%5Bslug%5D/page-5eca7ec39609f401.js\"],\"\"]\nf:T24d8,"])</script><script>self.__next_f.push([1,"\u003cdiv style=\"text-align: center\"\u003e\n\n\n\u003cp style=\"font-size: 0.8em; color: white; margin-bottom: 2em;\"\u003e\n  \u003ca href=\"https://www.cs-pk.com/\"\u003ePatrick Tser Jern Kon\u003c/a\u003e\u003csup\u003e*\u003c/sup\u003e, \n  \u003ca href=\"https://websites.umich.edu/~amberljc/\"\u003eJiachen Liu\u003c/a\u003e\u003csup\u003e*\u003c/sup\u003e (Equal Contribution\u003csup\u003e*\u003c/sup\u003e)\u003cbr\u003e\n  Xinyi Zhu,\n  Qiuyi Ding, \n  \u003ca href=\"https://www.linkedin.com/in/jingjia-peng/\"\u003eJingjia Peng\u003c/a\u003e (University of Michigan),  \n  \u003ca href=\"https://jxing.me/\"\u003eJiarong Xing\u003c/a\u003e (UC Berkeley \u0026amp; Rice University), \n  \u003ca href=\"https://huangyibo.github.io/\"\u003eYibo Huang\u003c/a\u003e (University of Michigan), \n  \u003ca href=\"https://yimingqiu.me/\"\u003eYiming Qiu\u003c/a\u003e (UC Berkeley \u0026amp; University of Michigan), \n  \u003ca href=\"https://scholar.google.com/citations?user=HtNfeKYAAAAJ\u0026amp;hl=en\"\u003eJayanth Srinivasa\u003c/a\u003e,\n  \u003ca href=\"https://www.linkedin.com/in/myungjin-lee-5308136/\"\u003eMyungjin Lee\u003c/a\u003e (Cisco Research), \u003cbr\u003e\n  \u003ca href=\"https://www.mosharaf.com/\"\u003eMosharaf Chowdhury\u003c/a\u003e (University of Michigan), \n  \u003ca href=\"https://stanford.edu/~matei/\"\u003eMatei Zaharia\u003c/a\u003e (UC Berkeley), \n  \u003ca href=\"https://web.eecs.umich.edu/~chenang/\"\u003eAng Chen\u003c/a\u003e (University of Michigan)\n\u003c/p\u003e\n\u003c/div\u003e\n\n\u003cdiv style=\"text-align: center; margin-bottom: 2em;\"\u003e\n  \u003cdiv style=\"margin-bottom: 2em;\"\u003e\n    \u003ca href=\"https://github.com/Just-Curieous/Curie/tree/main/benchmark/exp_bench\" style=\"margin: 0 10px; padding: 8px 15px; background-color: #f8f9fa; border-radius: 5px; text-decoration: none; color: #2c3e50;\"\u003e📊 Dataset\u003c/a\u003e\n    \u003ca href=\"https://arxiv.org/abs/2505.24785\" style=\"margin: 0 10px; padding: 8px 15px; background-color: #f8f9fa; border-radius: 5px; text-decoration: none; color: #2c3e50;\"\u003e📄 Paper\u003c/a\u003e\n    \u003ca href=\"https://github.com/Just-Curieous/Curie\" style=\"margin: 0 10px; padding: 8px 15px; background-color: #f8f9fa; border-radius: 5px; text-decoration: none; color: #2c3e50;\"\u003e💻 Github\u003c/a\u003e\n  \u003c/div\u003e\n\u003c/div\u003e\n\n\n\u003cp\u003eAI for Science is rapidly advancing, with promising early work on scientific automation—such as DeepMind's \u003ca href=\"https://deepmind.google/discover/blog/alphaevolve-a-gemini-powered-coding-agent-for-designing-advanced-algorithms/\"\u003eAlphaEvolve\u003c/a\u003e and others highlighted in this Nature \u003ca href=\"https://www.nature.com/articles/s41586-023-06221-2\"\u003epaper\u003c/a\u003e.\nA particularly exciting frontier is \u003cstrong\u003ethe automation of AI research experimentation\u003c/strong\u003e—the process of designing, executing, and analyzing experiments to advance AI itself. Unlike fields requiring physical experimentation, AI research is largely digital—well-suited for LLM-based automation. \u003c/p\u003e\n  \u003cdiv style=\"padding: 20px; border-radius: 8px; margin: 2em 0;\"\u003e\n    \u003cp style=\"margin-bottom: 1em;\"\u003eIdeally, we want to provide an AI agent with a research goal—such as reproducing a result, validating a new hypothesis, or testing an ablation— along with the specific context and have the agent:\u003c/p\u003e\n    \u003cul style=\"list-style-type: none; padding-left: 0;\"\u003e\n      \u003cli style=\"margin-bottom: 0.5em;\"\u003e• Formulate hypotheses, design experiments,\u003c/li\u003e\n      \u003cli style=\"margin-bottom: 0.5em;\"\u003e• Interpret the associated codebase and identify how to modify it,\u003c/li\u003e\n      \u003cli style=\"margin-bottom: 0.5em;\"\u003e• Configure and execute experiments under the right conditions,\u003c/li\u003e\n      \u003cli style=\"margin-bottom: 0.5em;\"\u003e• Analyze results and iteratively refine its approach based on findings\u003c/li\u003e\n    \u003c/ul\u003e\n  \u003c/div\u003e\n\n\u003cdiv style=\"text-align: center; max-width: 800px; margin: 0 auto; padding: 15px;\"\u003e\n    \u003cimg src=\"../blog/exp-bench-overview.png\" alt=\"EXP-Bench Overview\" style=\"width: 100%; height: auto; display: block; margin: 0 auto;\"\u003e\n    \u003cp style=\"font-size: 0.8em; margin-top: 10px; color: white;\"\u003e\n        Figure 1. EXP-Bench evaluates AI agents on research experiment tasks extracted semi-autonomously from peer-reviewed AI papers.\n    \u003c/p\u003e\n\u003c/div\u003e\nAchieving this vision requires benchmarks that evaluate agents in real-world research scenarios. But how do we define those scenarios in a way that’s representative, reproducible, and gradable?\n\n\n\n\u003cp\u003eIntuitively, peer-reviewed AI papers (e.g., in NeurIPS) along with their open-source codebases, offer a rich source of completed experiments that could, in theory, be repurposed to evaluate AI capabilities in research automation. In practice, however, extracting these tasks is difficult. Papers often present a polished narrative that omits intermediate steps, while critical details—such as the precise conditions under which results hold—are scattered across dense text, supplementary materials, and sprawling repositories.\u003c/p\u003e\n\u003ch2\u003eOur Contribution: EXP-Bench\u003c/h2\u003e\n\u003cp\u003eTo address this challenge, we introduce \u003cstrong\u003eEXP-Bench\u003c/strong\u003e, a new benchmark designed to make the ever-expanding landscape of published research more accessible for evaluating AI agents in \u003cstrong\u003econducting end-to-end AI research experiments\u003c/strong\u003e—from hypothesis to experimental setup to conclusion, as shown in Figure 1. We develop a semi-automated pipeline (Figure 2) that uses multimodal and agentic approaches to reconstruct experiments from fragmented and dense sources (e.g., coding agents identify setups by conditioning on ground-truth outcomes and leveraging the full codebase—reducing the task to a constrained search), while interleaving these steps with lightweight human validation to ensure correctness.\u003c/p\u003e\n\u003cp\u003eUsing this approach, we distilled \u003cem\u003e461 experiments from NeurIPS and ICLR papers\u003c/em\u003e—spanning domains such as vision, RL, and computational biology—resulting in over 12,000 gradable subtasks.\u003c/p\u003e\n\u003cdiv style=\"text-align: center; max-width: 800px; margin: 0 auto; padding: 15px;\"\u003e\n    \u003cimg src=\"../blog/exp-construction.png\" alt=\"EXP-Bench Construction\" style=\"width: 100%; height: auto; display: block; margin: 0 auto;\"\u003e\n    \u003cp style=\"font-size: 0.8em; margin-top: 10px; color: white;\"\u003e\n        Figure 2. Our semi-automated pipeline for constructing EXP-Bench from published papers.\n    \u003c/p\u003e\n    \u003cp style=\"font-size: 0.8em; margin-top: 15px; color: white;\"\u003e \n            See Figure 3 for an example AI research task extracted through this pipeline.\n    \u003c/p\u003e\n\n\u003c/div\u003e\n\u003cdiv style=\"text-align: center; max-width: 800px; margin: 0 auto; padding: 0 15px;\"\u003e \n    \u003cimg src=\"../blog/exp-bench-example.png\" alt=\"EXP-Bench Example\" style=\"width: 100%; height: auto; display: block; margin: 0 auto;\"\u003e\n    \u003cp style=\"font-size: 0.8em; margin-top: 10px; color: white;\"\u003e\n        Figure 3. One AI research task example from ICLR 2024 MogaNet [1].\n    \u003c/p\u003e\n\u003c/div\u003e\n\n\u003ch2\u003eWhat EXP-Bench Reveals About Today's AI Agents\u003c/h2\u003e\n\u003cp\u003eWe tested leading agents, including \u003cem\u003eOpenHands w/ Claude Sonnet 3.7\u003c/em\u003e, and found that while they can earn partial credit for individual steps like experiment design or coding (~20-35% success), their ability to \u003cem\u003ecomplete a full, executable experiment\u003c/em\u003e is nearly non-existent—a mere 0.5% success rate. \n  \u003c/p\u003e\u003cdiv style=\"padding: 20px; border-radius: 8px; margin: 2em 0;\"\u003e\n    \u003cp style=\"margin-bottom: 1em;\"\u003eOur analysis pinpointed several critical weaknesses:\u003c/p\u003e\n    \u003col style=\"padding-left: 20px;\"\u003e\n      \u003cli style=\"margin-bottom: 0.5em;\"\u003e\u003cem\u003eLimited Long-Horizon Planning and Reasoning\u003c/em\u003e\u003c/li\u003e\n      \u003cli style=\"margin-bottom: 0.5em;\"\u003e\u003cem\u003eInability to Handle Open-Ended and Ambiguous Tasks (35.9%)\u003c/em\u003e\u003c/li\u003e\n      \u003cli style=\"margin-bottom: 0.5em;\"\u003e\u003cem\u003eDifficulty with Environment Setup (41.3%) and Code Debugging (29.8%)\u003c/em\u003e\u003c/li\u003e\n    \u003c/ol\u003e\n  \u003c/div\u003e\u003cp\u003e\u003c/p\u003e\n\u003cp\u003eThese results highlight just how far we still are from our goal of automation of research experimentation. By identifying these bottlenecks and providing realistic step-by-step experiment procedures, EXP-Bench serves as a vital tool for future AI agents to improve their ability to conduct AI research experiments.\u003c/p\u003e\n\u003ch2\u003eLooking Forward\u003c/h2\u003e\n\u003cp\u003eThis work is, we hope, a small step toward our broader goal of designing agents capable of automating scientific research. We see EXP-Bench as a launchpad for the next wave of AI research copilots.\u003c/p\u003e\n\u003cp\u003eThat said, much work remains. While EXP-Bench currently focuses on machine learning papers, it does not yet address domains that require interaction with the physical world or support tasks involving true scientific invention. Expanding to those areas—and capturing the creativity, uncertainty, and iteration of real-world discovery—remains an open and exciting challenge.\u003c/p\u003e\n\u003ch2\u003eExplore Our Work\u003c/h2\u003e\n  \u003cdiv style=\"display: flex; justify-content: center; gap: 20px; margin: 2em 0;\"\u003e\n    \u003ca href=\"https://arxiv.org/abs/2505.24785\" style=\"padding: 10px 20px; background-color: #f8f9fa; color: #2c3e50; text-decoration: none; border-radius: 5px;\"\u003e📑 Full paper\u003c/a\u003e\n    \u003ca href=\"https://github.com/Just-Curieous/Curie/tree/main/benchmark/exp_bench\" style=\"padding: 10px 20px; background-color: #f8f9fa; color: #2c3e50; text-decoration: none; border-radius: 5px;\"\u003e🗃️ Open-sourced dataset\u003c/a\u003e\n  \u003c/div\u003e\n\n\u003cpre\u003e\u003ccode class=\"language-latex hljs\" data-highlighted=\"yes\"\u003e@article{kon2025expbenchaiconductai,\n      title={EXP-Bench: Can AI Conduct AI Research Experiments?}, \n      author={Patrick Tser Jern Kon and Jiachen Liu and Xinyi Zhu and Qiuyi Ding and Jingjia Peng and Jiarong Xing and Yibo Huang and Yiming Qiu and Jayanth Srinivasa and Myungjin Lee and Mosharaf Chowdhury and Matei Zaharia and Ang Chen},\n      journal={arXiv preprint 2505.24785}\n      year={2025},\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cem\u003e[1] MogaNet: Multi-order Gated Aggregation Network. ICLR 2024\u003c/em\u003e\u003c/p\u003e\n"])</script><script>self.__next_f.push([1,"7:[\"$\",\"article\",null,{\"className\":\"min-h-screen bg-stone-600\",\"children\":[[\"$\",\"header\",null,{\"className\":\"bg-stone-600\",\"children\":[\"$\",\"div\",null,{\"className\":\"max-w-4xl mx-auto px-4 sm:px-6 lg:px-8 pt-16\",\"children\":[[\"$\",\"div\",null,{\"className\":\"mb-4\",\"children\":[\"$\",\"$Le\",null,{\"href\":\"/\",\"className\":\"text-orange-400 font-medium\",\"children\":\"Home\"}]}],[\"$\",\"h1\",null,{\"className\":\"text-4xl sm:text-5xl font-bold text-white mb-6\",\"children\":\"Can AI Conduct AI Research Experiments?\"}],[\"$\",\"div\",null,{\"className\":\"flex items-center gap-4 text-white mb-6\",\"children\":[\"$\",\"time\",null,{\"dateTime\":\"$D2025-06-11T00:00:00.000Z\",\"children\":\"June 10, 2025\"}]}],[\"$\",\"div\",null,{\"className\":\"flex gap-2 flex-wrap mb-6\",\"children\":[[\"$\",\"span\",\"machine-learning\",{\"className\":\"px-3 py-1 bg-gray-200 text-gray-700 hover:bg-gray-300 text-sm rounded-full\",\"children\":\"machine-learning\"}],[\"$\",\"span\",\"research\",{\"className\":\"px-3 py-1 bg-gray-200 text-gray-700 hover:bg-gray-300 text-sm rounded-full\",\"children\":\"research\"}]]}]]}]}],[\"$\",\"div\",null,{\"className\":\"prose prose-lg max-w-4xl mx-auto markdown-content\",\"children\":[\"$\",\"div\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"$f\"}}]}]]}]\nb:[[\"$\",\"meta\",\"0\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}],[\"$\",\"meta\",\"1\",{\"charSet\":\"utf-8\"}],[\"$\",\"title\",\"2\",{\"children\":\"Curie Research Platform\"}],[\"$\",\"link\",\"3\",{\"rel\":\"icon\",\"href\":\"/favicon.ico\",\"type\":\"image/x-icon\",\"sizes\":\"200x200\"}],[\"$\",\"meta\",\"4\",{\"name\":\"next-size-adjust\"}]]\n6:null\n"])</script></body></html>